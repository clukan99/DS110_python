{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:\n",
    "https://medium.com/biaslyai/pytorch-introduction-to-neural-network-feedforward-neural-network-model-e7231cff47cb\n",
    "https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-create-a-neural-network-for-regression-with-pytorch.md\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing basic libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing PyTorch libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TV</th>\n",
       "      <th>radio</th>\n",
       "      <th>newspaper</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>230.1</td>\n",
       "      <td>37.8</td>\n",
       "      <td>69.2</td>\n",
       "      <td>22.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44.5</td>\n",
       "      <td>39.3</td>\n",
       "      <td>45.1</td>\n",
       "      <td>10.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17.2</td>\n",
       "      <td>45.9</td>\n",
       "      <td>69.3</td>\n",
       "      <td>9.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>151.5</td>\n",
       "      <td>41.3</td>\n",
       "      <td>58.5</td>\n",
       "      <td>18.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>180.8</td>\n",
       "      <td>10.8</td>\n",
       "      <td>58.4</td>\n",
       "      <td>12.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TV  radio  newspaper  sales\n",
       "0  230.1   37.8       69.2   22.1\n",
       "1   44.5   39.3       45.1   10.4\n",
       "2   17.2   45.9       69.3    9.3\n",
       "3  151.5   41.3       58.5   18.5\n",
       "4  180.8   10.8       58.4   12.9"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad_data = pd.read_csv(\"../../DATA/Advertising.csv\")\n",
    "ad_data = ad_data.drop('CompID', axis = 1)\n",
    "ad_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ad_data = ad_data.values\n",
    "ad_train, ad_test = train_test_split(ad_data, test_size= 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_train = torch.tensor(ad_train, dtype=torch.float)\n",
    "D_test = torch.tensor(ad_test, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = D_train[:,0:2].t()\n",
    "y_train = D_train[:,2].t()\n",
    "\n",
    "x_test = D_test[:,0:2].t()\n",
    "y_test = D_test[:,2].t()\n",
    "\n",
    "#X_train, X_test = torch.Tensor(X_train),torch.Tensor(X_test)\n",
    "#y_train, y_test = torch.Tensor(y_train),torch.Tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A = torch.randn((1, n), requires_grad=True)\n",
    "#b = torch.randn(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(torch.nn.Module):\n",
    "     def __init__(self, input_dim, output_dim):\n",
    "         super(LogisticRegression, self).__init__()\n",
    "         self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "     def forward(self, x):\n",
    "         outputs = torch.sigmoid(self.linear(x))\n",
    "         return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200000\n",
    "input_dim = 2 # Two inputs x1 and x2 \n",
    "output_dim = 1 # Single binary output \n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(input_dim,output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "losses_test = []\n",
    "Iterations = []\n",
    "iter = 0\n",
    "for epoch in tqdm(range(int(epochs)),desc='Training Epochs'):\n",
    "    x = X_train\n",
    "    labels = y_train\n",
    "    optimizer.zero_grad() # Setting our stored gradients equal to zero\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(torch.squeeze(outputs), labels) \n",
    "    \n",
    "    loss.backward() # Computes the gradient of the given tensor w.r.t. the weights/bias\n",
    "    \n",
    "    optimizer.step() # Updates weights and biases with the optimizer (SGD)\n",
    "    \n",
    "    iter+=1\n",
    "    if iter%10000==0:\n",
    "        with torch.no_grad():\n",
    "            # Calculating the loss and accuracy for the test dataset\n",
    "            correct_test = 0\n",
    "            total_test = 0\n",
    "            outputs_test = torch.squeeze(model(X_test))\n",
    "            loss_test = criterion(outputs_test, y_test)\n",
    "            \n",
    "            predicted_test = outputs_test.round().detach().numpy()\n",
    "            total_test += y_test.size(0)\n",
    "            correct_test += np.sum(predicted_test == y_test.detach().numpy())\n",
    "            accuracy_test = 100 * correct_test/total_test\n",
    "            losses_test.append(loss_test.item())\n",
    "            \n",
    "            # Calculating the loss and accuracy for the train dataset\n",
    "            total = 0\n",
    "            correct = 0\n",
    "            total += y_train.size(0)\n",
    "            correct += np.sum(torch.squeeze(outputs).round().detach().numpy() == y_train.detach().numpy())\n",
    "            accuracy = 100 * correct/total\n",
    "            losses.append(loss.item())\n",
    "            Iterations.append(iter)\n",
    "            \n",
    "            print(f\"Iteration: {iter}. \\nTest - Loss: {loss_test.item()}. Accuracy: {accuracy_test}\")\n",
    "            print(f\"Train -  Loss: {loss.item()}. Accuracy: {accuracy}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = 1\n",
    "x2 = 1\n",
    "new_data = torch.tensor([x1,x2]).type(torch.FloatTensor)\n",
    "with torch.no_grad():\n",
    "    prediction = model(new_data).round()\n",
    "    if prediction == 1.0:\n",
    "        print(f'The model classifies this point as RED')\n",
    "    else:\n",
    "        print(f'The model classifies this point as BLUE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All of it at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "matplotlib.rc('text', usetex=True)\n",
    "matplotlib.rcParams['text.latex.preamble']=[r\"\\usepackage{amsmath}\"]\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "separable = False\n",
    "while not separable:\n",
    "    samples = make_classification(n_samples=1000, n_features=2, n_redundant=0, n_informative=1, n_clusters_per_class=1, flip_y=-1)\n",
    "    red = samples[0][samples[1] == 0]\n",
    "    blue = samples[0][samples[1] == 1]\n",
    "    separable = any([red[:, k].max() < blue[:, k].min() or red[:, k].min() > blue[:, k].max() for k in range(2)])\n",
    "\n",
    "\n",
    "red_labels = np.zeros(len(red))\n",
    "blue_labels = np.ones(len(blue))\n",
    "\n",
    "labels = np.append(red_labels,blue_labels)\n",
    "inputs = np.concatenate((red,blue),axis=0)\n",
    "\n",
    "X_train, X_test, y_train,  y_test = train_test_split(\n",
    "    inputs, labels, test_size=0.33, random_state=42)\n",
    "\n",
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        outputs = torch.sigmoid(self.linear(x))\n",
    "        return outputs\n",
    "\n",
    "epochs = 200_000\n",
    "input_dim = 2 # Two inputs x1 and x2 \n",
    "output_dim = 1 # Two possible outputs\n",
    "learning_rate = 0.01\n",
    "\n",
    "model = LogisticRegression(input_dim,output_dim)\n",
    "\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "X_train, X_test = torch.Tensor(X_train),torch.Tensor(X_test)\n",
    "y_train, y_test = torch.Tensor(y_train),torch.Tensor(y_test)\n",
    "\n",
    "losses = []\n",
    "losses_test = []\n",
    "Iterations = []\n",
    "iter = 0\n",
    "for epoch in tqdm(range(int(epochs)),desc='Training Epochs'):\n",
    "    x = X_train\n",
    "    labels = y_train\n",
    "    optimizer.zero_grad() # Setting our stored gradients equal to zero\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(torch.squeeze(outputs), labels) # [200,1] -squeeze-> [200]\n",
    "    \n",
    "    loss.backward() # Computes the gradient of the given tensor w.r.t. graph leaves \n",
    "    \n",
    "    optimizer.step() # Updates weights and biases with the optimizer (SGD)\n",
    "    \n",
    "    iter+=1\n",
    "    if iter%10000==0:\n",
    "        # calculate Accuracy\n",
    "        with torch.no_grad():\n",
    "            # Calculating the loss and accuracy for the test dataset\n",
    "            correct_test = 0\n",
    "            total_test = 0\n",
    "            outputs_test = torch.squeeze(model(X_test))\n",
    "            loss_test = criterion(outputs_test, y_test)\n",
    "            \n",
    "            predicted_test = outputs_test.round().detach().numpy()\n",
    "            total_test += y_test.size(0)\n",
    "            correct_test += np.sum(predicted_test == y_test.detach().numpy())\n",
    "            accuracy_test = 100 * correct_test/total_test\n",
    "            losses_test.append(loss_test.item())\n",
    "            \n",
    "            # Calculating the loss and accuracy for the train dataset\n",
    "            total = 0\n",
    "            correct = 0\n",
    "            total += y_train.size(0)\n",
    "            correct += np.sum(torch.squeeze(outputs).round().detach().numpy() == y_train.detach().numpy())\n",
    "            accuracy = 100 * correct/total\n",
    "            losses.append(loss.item())\n",
    "            Iterations.append(iter)\n",
    "            \n",
    "            print(f\"Iteration: {iter}. \\nTest - Loss: {loss_test.item()}. Accuracy: {accuracy_test}\")\n",
    "            print(f\"Train -  Loss: {loss.item()}. Accuracy: {accuracy}\\n\")\n",
    "\n",
    "\n",
    "def model_plot(model,X,y,title):\n",
    "    parm = {}\n",
    "    b = []\n",
    "    for name, param in model.named_parameters():\n",
    "        parm[name]=param.detach().numpy()  \n",
    "    \n",
    "    w = parm['linear.weight'][0]\n",
    "    b = parm['linear.bias'][0]\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y,cmap='jet')\n",
    "    u = np.linspace(X[:, 0].min(), X[:, 0].max(), 2)\n",
    "    plt.plot(u, (0.5-b-w[0]*u)/w[1])\n",
    "    plt.xlim(X[:, 0].min()-0.5, X[:, 0].max()+0.5)\n",
    "    plt.ylim(X[:, 1].min()-0.5, X[:, 1].max()+0.5)\n",
    "    plt.xlabel(r'$\\boldsymbol{x_1}$',fontsize=16) # Normally you can just add the argument fontweight='bold' but it does not work with latex\n",
    "    plt.ylabel(r'$\\boldsymbol{x_2}$',fontsize=16)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Train Data\n",
    "model_plot(model,X_train,y_train,'Train Data')\n",
    "\n",
    "# Test Dataset Results\n",
    "model_plot(model,X_test,y_test,'Test Data')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3edbd7b23b0fac25ed01096b34e5f59511cb659c0b1d4bb21fa4d1eb2f403c11"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
